import requests
import json

# --- å‡†å¤‡å·¥ä½œ ---

# 1. å®šä¹‰ Ollama çš„ API åœ°å€
OLLAMA_ENDPOINT = "http://localhost:11434/api/chat"

# *** æ–°å¢ï¼šè§£å†³ noproxy é—®é¢˜çš„å…³é”® ***
# å‘Šè¯‰ requests åº“ä¸è¦ä¸º http æˆ– https è¯·æ±‚ä½¿ç”¨ä»»ä½•ä»£ç†
# è¿™ä¼šè¦†ç›–æ‰ç³»ç»Ÿç¯å¢ƒå˜é‡ä¸­çš„ http_proxy/https_proxy
proxies_to_use = {
    "http": None,
    "https": None
}

# 2. å®šä¹‰æˆ‘ä»¬çš„â€œå·¥å…·è¯´æ˜ä¹¦â€ (JSON Schema)
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_country_info",
            "description": "æ ¹æ®ä¸€ä¸ªå›½å®¶çš„è‹±æ–‡æˆ–å®˜æ–¹åç§°ï¼Œä» restcountries.com API è·å–è¯¥å›½çš„è¯¦ç»†ä¿¡æ¯ï¼Œå¦‚é¦–éƒ½ã€äººå£ã€è´§å¸ç­‰ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "country_name": {
                        "type": "string",
                        "description": "è¦æŸ¥è¯¢çš„å›½å®¶çš„è‹±æ–‡åç§°ï¼Œä¾‹å¦‚: Germany, France, Japan"
                    }
                },
                "required": ["country_name"]
            }
        }
    }
]

# 3. æˆ‘ä»¬çš„åˆå§‹é—®é¢˜
user_prompt = "å¸®æˆ‘æŸ¥è¯¢ä¸€ä¸‹å¾·å›½çš„å›½å®¶ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯å®ƒçš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ"

# 4. å‡†å¤‡å¥½æˆ‘ä»¬çš„â€œåŒæ‰‹â€ï¼ˆæ‰§è¡Œå™¨ï¼‰
def execute_function_call(function_name, arguments):
    """
    ä¸€ä¸ªâ€œæ‰§è¡Œå™¨â€å‡½æ•°ï¼Œè´Ÿè´£æ˜ å°„ AI çš„å†³ç­–åˆ°çœŸå®çš„ç½‘ç»œè¯·æ±‚ã€‚
    """
    if function_name == "get_country_info":
        print(f"âœ‹ æ­£åœ¨æ‰§è¡Œå·¥å…·ï¼šæŸ¥è¯¢ {arguments['country_name']}...")
        try:
            country_name = arguments['country_name']
            api_url = f"https://restcountries.com/v3.1/name/{country_name}"
            
            # *** ä¿®æ”¹ï¼šåœ¨è¿™é‡Œä¹Ÿä¸ºå¤–éƒ¨ API è°ƒç”¨æ·»åŠ ä»£ç†è®¾ç½® ***
            # æˆ‘ä»¬å‡è®¾å¤–éƒ¨ API (restcountries.com) *éœ€è¦* èµ°ä»£ç†
            # å¦‚æœå®ƒä¹Ÿä¸éœ€è¦ä»£ç†ï¼Œä¹Ÿä½¿ç”¨ proxies_to_use
            # å¦‚æœå®ƒéœ€è¦ä»£ç†ï¼Œè€Œä½ çš„ç¯å¢ƒå˜é‡æ˜¯å¯¹çš„ï¼Œè¿™é‡Œå°±ä»€ä¹ˆéƒ½ä¸ç”¨åŠ 
            # ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾å¤–éƒ¨ API ä¹Ÿä¸èµ°ä»£ç†ï¼š
            response = requests.get(api_url, proxies=proxies_to_use) 
            
            response.raise_for_status() 
            api_response_json = response.json()
            capital = api_response_json[0].get("capital", [None])[0]
            population = api_response_json[0].get("population", None)
            
            print(f"âœ… å·¥å…·æ‰§è¡Œå®Œæ¯•ã€‚")
            
            result = {
                "capital": capital,
                "population": population
            }
            return json.dumps(result)
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: {e}")
            return json.dumps({"error": str(e)})
    else:
        print(f"ğŸ¤· AI å†³å®šè°ƒç”¨ä¸€ä¸ªæˆ‘ä¸è®¤è¯†çš„å·¥å…·: {function_name}")
        return None

# --- Function Calling æµç¨‹å¼€å§‹ ---

def run_function_call_loop():
    
    messages = [
        {
            "role": "user",
            "content": user_prompt
        }
    ]

    # --- æ­¥éª¤ 1: (AI å†³ç­–) ---
    print("ğŸ¤– æ­£åœ¨è¯¢é—® AI (Ollama)...")
    
    payload = {
        "model": "qwen3:4b",
        "messages": messages,
        "tools": tools,
        "stream": False
    }

    try:
        # *** ä¿®æ”¹ï¼šåœ¨è¿™é‡Œæ·»åŠ  proxies=proxies_to_use ***
        response_step1 = requests.post(OLLAMA_ENDPOINT, json=payload, proxies=proxies_to_use)
        
        response_step1.raise_for_status()
        ai_message = response_step1.json().get('message', {})
        messages.append(ai_message)

    except requests.exceptions.ConnectionError:
        print(f"âŒ è¿æ¥ Ollama å¤±è´¥ (127.0.0.1:7897?)ã€‚è¯·ç¡®ä¿ Ollama æœåŠ¡å™¨æ­£åœ¨è¿è¡Œï¼š`ollama serve`")
        return
    except Exception as e:
        print(f"âŒ æ­¥éª¤ 1 å‘ç”Ÿé”™è¯¯: {e}")
        return

    # --- æ­¥éª¤ 2: (è„šæœ¬æ‰§è¡Œ) ---
    
    if not ai_message.get("tool_calls"):
        print("ğŸ§  AI å†³å®šä¸ä½¿ç”¨å·¥å…·ï¼Œç›´æ¥å›ç­”:")
        print(ai_message.get('content'))
        return

    print("ğŸ§  AI å†³å®šè°ƒç”¨ä¸€ä¸ªå·¥å…·...")
    
    tool_calls = ai_message.get("tool_calls", [])
    tool_results = []

    for tool_call in tool_calls:
        function_name = tool_call['function']['name']
        arguments = tool_call['function']['arguments'] 
        
        tool_result_content = execute_function_call(function_name, arguments)
        
        if tool_result_content:
            tool_results.append({
                "role": "tool",
                "tool_call_id": tool_call.get('id'),
                "content": tool_result_content
            })

    if not tool_results:
        print("ğŸ¤· å·¥å…·æ‰§è¡Œå¤±è´¥ï¼Œæ— æ³•ç»§ç»­ã€‚")
        return

    messages.extend(tool_results)

    # --- æ­¥éª¤ 3: (è¿”å›ç»™ AI å“åº”) ---
    print("ğŸ¤– æ­£åœ¨å°†å·¥å…·ç»“æœå‘å› AIï¼Œä»¥ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ...")

    final_payload = {
        "model": "qwen3:4b",
        "messages": messages,
        "stream": False
    }

    try:
        # *** ä¿®æ”¹ï¼šåœ¨è¿™é‡Œä¹Ÿæ·»åŠ  proxies=proxies_to_use ***
        response_step3 = requests.post(OLLAMA_ENDPOINT, json=final_payload, proxies=proxies_to_use)
        
        response_step3.raise_for_status()
        final_message = response_step3.json().get('message', {})
        
        print("\n--- æœ€ç»ˆç­”æ¡ˆ ---")
        print(final_message.get('content'))
        
    except Exception as e:
        print(f"âŒ æ­¥éª¤ 3 å‘ç”Ÿé”™è¯¯: {e}")

# --- è¿è¡Œä¸»å‡½æ•° ---
if __name__ == "__main__":
    run_function_call_loop()
